# 本地化模型接入指南

> Bibliothecarius初始化提供了openai的`chat`模型进行对话，您也可以引入自己模型，请参考下文。

## 1.实现本地接口

Bibliothecarius发起请求

- `POST`

- `Content-Type: application/json`

- 参数：

  ```json
  {
  	"input":"组装之后的用户问题",
    //历史记录
    "chatContextList":[
      {
        //排序
        "sort": 0,
        "user": "用户问题",
        "assistant": "ai输出"
      }
    ],
    //用户配置参数
    "params":{}
  }
  ```

您的本地服务需要返回结果：

```
string
```

## 2.接入

> 您可以使用API创建，也可以使用可视化界面创建。

### 使用API创建接口

请参考[接口文档](https://apifox.com/apidoc/shared-0dfab7c9-3d3f-498a-b4c2-88b5e6b99a01/api-71770815)

请求接口`POST:/external/model`

参数

```json
{
    "chatAddress": "chat接口地址",
    "name": "名称(唯一且不可和系统内置重复)",
    "checkParametersAddress": "参数检查接口（为空则不检查参数）",
    "remark": "简介",
  	// 允许最大请求输入
    "inputMaxToken": 53
}
```

### 使用可视化界面创建

#TODO 前端尚未实现

## 3.示例

-  [ChatGLM-6B接入.md](ChatGLM-6B接入.md) 